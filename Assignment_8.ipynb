{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRyZeRnlOxVl5KvDnUxs5v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Computer-Vision/blob/main/Assignment_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Using our own terms and diagrams, explain INCEPTIONNET ARCHITECTURE.\n",
        "\n",
        "The inception layer is the core concept of a sparsely connected architecture.1Ã—1 Convolutional layer before applying another layer, which is mainly used for dimensionality reduction. A parallel Max Pooling layer, which provides another option to the inception layer"
      ],
      "metadata": {
        "id": "JqknzWTWCe5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Describe the Inception block.\n",
        "\n",
        "An Inception Module is an image model block that aims to approximate an optimal local sparse structure in a CNN. Put simply, it allows for us to use multiple types of filter size, instead of being restricted to a single filter size, in a single image block, which we then concatenate and pass onto the next layer."
      ],
      "metadata": {
        "id": "ZGoNXWVxCrFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is the DIMENSIONALITY REDUCTION LAYER (1 LAYER CONVOLUTIONAL)?\n",
        "\n",
        "Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset. More input features often make a predictive modeling task more challenging to model, more generally referred to as the curse of dimensionality."
      ],
      "metadata": {
        "id": "GUCCE8beCzRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.THE IMPACT OF REDUCING DIMENSIONALITY ON NETWORK PERFORMANCE\n",
        "\n",
        "It reduces the time and storage space required. The removal of multicollinearity improves the interpretation of the parameters of the machine learning model. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D. Reduce space complexity."
      ],
      "metadata": {
        "id": "K5ocZ1hXC9Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Mention three components. Style GoogLeNet\n",
        "\n",
        "GoogLeNet is a convolutional neural network that is 22 layers deep. You can load a pretrained version of the network trained on either the ImageNet [1] or Places365 [2] [3] data sets. The network trained on ImageNet classifies images into 1000 object categories, such as keyboard, mouse, pencil, and many animals."
      ],
      "metadata": {
        "id": "lkW332zCDMBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Using our own terms and diagrams, explain RESNET ARCHITECTURE.\n",
        "\n",
        "ResNet architecture uses the CNN blocks multiple times, so let us create a class for CNN block, which takes input channels and output channels. ... Then create a ResNet class that takes the input of a number of blocks, layers, image channels, and the number of classes."
      ],
      "metadata": {
        "id": "rOiI0TtuDUaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What do Skip Connections entail?\n",
        "\n",
        "Skip Connections (or Shortcut Connections) as the name suggests skips some of the layers in the neural network and feeds the output of one layer as the input to the next layers. Skip Connections were introduced to solve different problems in different architectures."
      ],
      "metadata": {
        "id": "dW9gPwL8DcUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is the definition of a residual Block?\n",
        "\n",
        "A residual block is a stack of layers set in such a way that the output of a layer is taken and added to another layer deeper in the block. The non-linearity is then applied after adding it together with the output of the corresponding layer in the main path\n",
        "\n"
      ],
      "metadata": {
        "id": "XzRT_8KoDsOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.How can transfer learning help with problems?\n",
        "\n",
        "No knowledge is retained which can be transferred from one model to another. In transfer learning, you can leverage knowledge (features, weights etc) from previously trained models for training newer models and even tackle problems like having less data for the newer task."
      ],
      "metadata": {
        "id": "oTZGgaswEEeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What is transfer learning, and how does it work?\n",
        "\n",
        "Transfer learning is an optimization that allows rapid progress or improved performance when modeling the second task. Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned.\n",
        "\n"
      ],
      "metadata": {
        "id": "RLhDD3oGD3a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.HOW DO NEURAL NETWORKS LEARN FEATURES?\n",
        "\n",
        "CNNs use convolutional layers to extract features and use pooling (max or average) layers to generalize features. The set of the various filters they used for Convolutional Layers extract different sets of features.\n",
        "\n"
      ],
      "metadata": {
        "id": "KzAhoNZfEeYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.WHY IS FINE-TUNING BETTER THAN START-UP TRAINING?\n",
        "\n",
        "Fine-tuning means taking some machine learning model that has already learned something before (i.e. been trained on some data) and then training that model (i.e. training it some more, possibly on different data). That's all fine-tuning means."
      ],
      "metadata": {
        "id": "BGJIit-kErER"
      }
    }
  ]
}