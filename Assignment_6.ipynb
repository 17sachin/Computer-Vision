{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgC1NyOVn9JZhw4DRbIjpm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Computer-Vision/blob/main/Assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?\n",
        "\n",
        "By default, all weights in a keras model are trainable. When you make them untrainable, the algorithm will not update these weights anymore. This is useful, for instance, when you want a convolutional layer with a specific filter, like a Sobel filter, for instance."
      ],
      "metadata": {
        "id": "aRsvkQbhuhN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.In the CNN architecture, where does the DROPOUT LAYER go?\n",
        "\n",
        "Dropout may be implemented on any or all hidden layers in the network as well as the visible or input layer. It is not used on the output layer. The term “dropout” refers to dropping out units (hidden and visible) in a neural network. — Dropout: A Simple Way to Prevent Neural Networks from Overfitting."
      ],
      "metadata": {
        "id": "csJPLGtMzjjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is the optimal number of hidden layers to stack?\n",
        "\n",
        "There is currently no theoretical reason to use neural networks with any more than two hidden layers. In fact, for many practical problems, there is no reason to use any more than one hidden layer."
      ],
      "metadata": {
        "id": "YHhx2jZ7z6bU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.In each layer, how many secret units or filters should there be?\n",
        "\n",
        "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."
      ],
      "metadata": {
        "id": "vevXhJgX0MJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What should your initial learning rate be?\n",
        "\n",
        "Conversely, larger learning rates will require fewer training epochs. Further, smaller batch sizes are better suited to smaller learning rates given the noisy estimate of the error gradient. A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem"
      ],
      "metadata": {
        "id": "mjoZ44Z10cbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What do you do with the activation function?\n",
        "\n",
        "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."
      ],
      "metadata": {
        "id": "_TJ-4AgQ0pkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What is NORMALIZATION OF DATA?\n",
        "\n",
        "Database normalization is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity."
      ],
      "metadata": {
        "id": "ZpQvJx4J0xpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is IMAGE AUGMENTATION and how does it work?\n",
        "\n",
        "Image augmentation is a technique of altering the existing data to create some more data for the model training process. In other words, it is the process of artificially expanding the available dataset for training a deep learning model."
      ],
      "metadata": {
        "id": "-LOBWPIc04Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is DECLINE IN LEARNING RATE?\n",
        "\n",
        "Learning rate decay is a  technique for training modern neural networks. It starts with a large learning rate and then decays it multiple times. It is empirically observed to help both optimization and generalization."
      ],
      "metadata": {
        "id": "pnVQfTYD1BLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What does EARLY STOPPING CRITERIA mean?\n",
        "\n",
        "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. ... Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit."
      ],
      "metadata": {
        "id": "AkveufGj1MT6"
      }
    }
  ]
}