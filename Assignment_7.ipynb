{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPcExZLO+O5hJjBvSQubc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/17sachin/Computer-Vision/blob/main/Assignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is the COVARIATE SHIFT Issue, and how does it affect you?\n",
        "\n",
        "Covariate shift occurs when the distribution of variables in the training data is different to real-world or testing data. This means that the model may make the wrong predictions once it is deployed, and its accuracy will be significantly lower."
      ],
      "metadata": {
        "id": "jeFqhD2z_2Zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the process of BATCH NORMALIZATION?\n",
        "\n",
        "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks."
      ],
      "metadata": {
        "id": "c7RzsnJD_9Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Using our own terms and diagrams, explain LENET ARCHITECTURE.\n",
        "\n",
        "LeNet is a convolutional neural network structure proposed by Yann LeCun et al. Convolutional neural networks are a kind of feed-forward neural network whose artificial neurons can respond to a part of the surrounding cells in the coverage range and perform well in large-scale image processing."
      ],
      "metadata": {
        "id": "NI2ku-ddAElN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Using our own terms and diagrams, explain ALEXNET ARCHITECTURE.\n",
        "\n",
        "AlexNet was the first convolutional network which used GPU to boost performance. 1. AlexNet architecture consists of 5 convolutional layers, 3 max-pooling layers, 2 normalization layers, 2 fully connected layers, and 1 softmax layer.\n",
        "\n",
        "AlexNet is a convolutional neural network that is 8 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database [1]. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals."
      ],
      "metadata": {
        "id": "rvegufPwAOib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Describe the vanishing gradient problem.\n",
        "\n",
        "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."
      ],
      "metadata": {
        "id": "IWsbUbYnAZPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is NORMALIZATION OF LOCAL RESPONSE?\n",
        "\n",
        "LRN is a non-trainable layer that square-normalizes the pixel values in a feature map within a local neighborhood. There are two types of LRN based on the neighborhood defined and can be seen in the figure below."
      ],
      "metadata": {
        "id": "XpAQW2JjAkXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.In AlexNet, what WEIGHT REGULARIZATION was used?\n",
        "\n",
        "It is common to use small values for the regularization hyperparameter that controls the contribution of each weight to the penalty. Perhaps start by testing values on a log scale, such as 0.1, 0.001, and 0.0001. Then use a grid search at the order of magnitude that shows the most promise."
      ],
      "metadata": {
        "id": "x956a3uPBF5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Using our own terms and diagrams, explain VGGNET ARCHITECTURE.\n",
        "\n",
        "VGGNet is a Convolutional Neural Network architecture proposed by Karen Simonyan and Andrew Zisserman from the University of Oxford in 2014.You can find the original paper of VGGNet which is titled Very Deep Convolutional Networks for Large Scale Image Recognition"
      ],
      "metadata": {
        "id": "f86U0klbBNkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Describe VGGNET CONFIGURATIONS.\n",
        "\n",
        "VGGNet is a neural network that performed very well in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2014. It scored first place on the image localization task and second place on the image classification task.Every year the people who run ImageNet host an image recognition competition."
      ],
      "metadata": {
        "id": "7G1XMvQnBjaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What regularization methods are used in VGGNET to prevent overfitting?\n",
        "\n",
        "Use Dropouts. Dropout is a regularization technique that prevents neural networks from overfitting. Regularization methods like L1 and L2 reduce overfitting by modifying the cost function."
      ],
      "metadata": {
        "id": "mejqpwpIBv2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "shfq3Hr_CDQU"
      }
    }
  ]
}